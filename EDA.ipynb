{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CV](https://www.cdc.gov/brfss/annual_data/2015/pdf/2015_Calculated_Variables_Version4_08_10_17-508c.pdf)\n",
    "(all_feat)[https://www.cdc.gov/brfss/annual_data/2015/pdf/codebook15_llcp.pdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.helpers import *\n",
    "from functions.implementations import *\n",
    "from functions.my_functions import *\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import df\n",
    "df = pd.read_csv(\"data/dataset/x_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ACTIN11_</th>\n",
       "      <th>ACTIN21_</th>\n",
       "      <th>DROCDY3_</th>\n",
       "      <th>FC60_</th>\n",
       "      <th>FRUTDA1_</th>\n",
       "      <th>_FRTRESP</th>\n",
       "      <th>_FRUITEX</th>\n",
       "      <th>_FRUTSUM</th>\n",
       "      <th>_HCVU651</th>\n",
       "      <th>...</th>\n",
       "      <th>_BMI5</th>\n",
       "      <th>_CASTHM1</th>\n",
       "      <th>_CHLDCNT</th>\n",
       "      <th>_CHOLCHK</th>\n",
       "      <th>_DRDXAR1</th>\n",
       "      <th>_DRNKWEK</th>\n",
       "      <th>_EDUCAG</th>\n",
       "      <th>_FLSHOT6</th>\n",
       "      <th>_FRT16</th>\n",
       "      <th>_FRTLT1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>20.78</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>4.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>28.70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>99900</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.03</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>27.96</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>900</td>\n",
       "      <td>999.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>24.39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>99900</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328130</th>\n",
       "      <td>328130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71</td>\n",
       "      <td>4.91</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>24.69</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328131</th>\n",
       "      <td>328131</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>24.39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328132</th>\n",
       "      <td>328132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>58.53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328133</th>\n",
       "      <td>328133</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.23</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>35.87</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328134</th>\n",
       "      <td>328134</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.99</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>22.81</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>328135 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id  ACTIN11_  ACTIN21_  DROCDY3_   FC60_  FRUTDA1_  _FRTRESP  \\\n",
       "0            0       1.0       1.0         0    4.61      0.71         1   \n",
       "1            1       NaN       NaN       100    4.91       NaN         0   \n",
       "2            2       1.0       0.0        13    4.11      2.00         1   \n",
       "3            3       2.0       2.0         0    4.11      0.00         1   \n",
       "4            4       NaN       NaN       900  999.00      0.50         1   \n",
       "...        ...       ...       ...       ...     ...       ...       ...   \n",
       "328130  328130       1.0       1.0        71    4.91      1.00         1   \n",
       "328131  328131       2.0       2.0         0    5.10      1.00         1   \n",
       "328132  328132       1.0       0.0         0    3.92      1.00         1   \n",
       "328133  328133       2.0       0.0         0    6.23      0.07         1   \n",
       "328134  328134       1.0       0.0         0    4.99      3.00         1   \n",
       "\n",
       "        _FRUITEX  _FRUTSUM  _HCVU651  ...  _BMI5  _CASTHM1  _CHLDCNT  \\\n",
       "0              0      0.71         1  ...  20.78         1         1   \n",
       "1              1       NaN         1  ...  28.70         1         1   \n",
       "2              0      2.03         9  ...    NaN         1         1   \n",
       "3              0      0.00         9  ...  27.96         1         1   \n",
       "4              0      1.50         9  ...  24.39         1         1   \n",
       "...          ...       ...       ...  ...    ...       ...       ...   \n",
       "328130         0      1.00         1  ...  24.69         1         1   \n",
       "328131         0      1.00         1  ...  24.39         1         1   \n",
       "328132         0      1.00         9  ...  58.53         1         1   \n",
       "328133         0      0.07         1  ...  35.87         2         1   \n",
       "328134         0      4.00         1  ...  22.81         1         2   \n",
       "\n",
       "        _CHOLCHK  _DRDXAR1  _DRNKWEK  _EDUCAG  _FLSHOT6  _FRT16  _FRTLT1  \n",
       "0              1       2.0         0        3       NaN       1        2  \n",
       "1              1       2.0     99900        2       NaN       1        9  \n",
       "2              1       1.0        93        4       1.0       1        1  \n",
       "3              2       1.0         0        2       2.0       1        2  \n",
       "4              1       2.0     99900        9       9.0       1        1  \n",
       "...          ...       ...       ...      ...       ...     ...      ...  \n",
       "328130         1       2.0      1000        4       NaN       1        1  \n",
       "328131         9       2.0         0        3       NaN       1        1  \n",
       "328132         1       2.0         0        3       2.0       1        1  \n",
       "328133         1       2.0         0        2       NaN       1        2  \n",
       "328134         1       1.0         0        4       NaN       1        1  \n",
       "\n",
       "[328135 rows x 80 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select columns that we are going to use for the analysis\n",
    "\n",
    "data_dict = {\n",
    "    # \"name of the var\": [\"synbol for nan\", type of varibale]\n",
    "    # if \"nan\" --> means to create a new class for missing values\n",
    "    # if \"no_missing\" --> no need for NaN impuation\n",
    "    # if \"countousn\" --> put mean\n",
    "    # if \"N\" --> trnasg all NaN in that N\n",
    "    \"ACTIN11_\": [\"nan\", \"discrete\"],\n",
    "    \"ACTIN21_\": [\"nan\", \"discrete\"],\n",
    "    \"DROCDY3_\": [900, \"continous\"],\n",
    "    \"FC60_\": [99900, \"continous\"],\n",
    "    \"FRUTDA1_\": [\"nan\", \"continous\"],\n",
    "    \"_FRTRESP\": [\"no_missing\", \"discrete\"],\n",
    "    \"_FRUITEX\": [\"no_missing\", \"discrete\"],\n",
    "    \"_FRUTSUM\": [\"nan\", \"continous\"],\n",
    "    \"_HCVU651\": [9, \"discrete\"],\n",
    "    \"_HISPANC\": [9, \"discrete\"],\n",
    "    \"_INCOMG\": [9, \"discrete\"],\n",
    "    \"_LMTACT1\": [9, \"discrete\"],\n",
    "    \"_LMTSCL1\": [9, \"discrete\"],\n",
    "    \"_LMTWRK1\": [9, \"discrete\"],\n",
    "    \"_LTASTH1\": [9, \"discrete\"],\n",
    "    \"_MINAC11\": [\"nan\", \"continous\"],\n",
    "    \"_MINAC21\": [\"nan\", \"continous\"],\n",
    "    \"_MISFRTN\": [\"no_missing\", \"discrete\"],\n",
    "    \"_MISVEGN\": [\"no_missing\", \"discrete\"],\n",
    "    \"_MRACE1\": [\"no_missing\", \"discrete\"],\n",
    "    \"_PA30021\": [9, \"discrete\"],\n",
    "    \"_PA150R2\": [9, \"discrete\"],\n",
    "    \"_PA300R2\": [9, \"discrete\"],\n",
    "    \"_PACAT1\": [9, \"discrete\"],\n",
    "    \"_PAINDX1\": [9, \"discrete\"],\n",
    "    \"_PAREC1\": [9, \"discrete\"],\n",
    "    \"_PASTAE1\": [9, \"discrete\"],\n",
    "    \"_PASTRNG\": [9, \"discrete\"],\n",
    "    \"_PNEUMO2\": [9, \"discrete\"],\n",
    "    \"_PRACE1\": [99, \"discrete\"],\n",
    "    \"_RACE\": [9, \"discrete\"],\n",
    "    \"_RACEG21\": [9, \"discrete\"],\n",
    "    \"_RACEGR3\": [9, \"discrete\"],\n",
    "    \"_RACE_G1\": [\"nan\", \"discrete\"],\n",
    "    \"_RFBING5\": [9, \"discrete\"],\n",
    "    \"_RFBMI5\": [9, \"discrete\"],\n",
    "    \"_RFCHOL\": [9, \"discrete\"],\n",
    "    \"_RFDRHV5\": [9, \"discrete\"],\n",
    "    \"_RFHLTH\": [9, \"discrete\"],\n",
    "    \"_RFHYPE5\": [9, \"discrete\"],\n",
    "    \"_RFSEAT2\": [9, \"discrete\"],\n",
    "    \"_RFSEAT3\": [9, \"discrete\"],\n",
    "    \"_RFSMOK3\": [9, \"discrete\"],\n",
    "    \"_SMOKER3\": [9, \"discrete\"],\n",
    "    \"_TOTINDA\": [9, \"discrete\"],\n",
    "    \"_VEG23\": [0, \"discrete\"],\n",
    "    \"_VEGESUM\": [\"nan\", \"continous\"],\n",
    "    \"_VEGETEX\": [1, \"discrete\"],\n",
    "    \"_VEGLT1\": [9, \"discrete\"],\n",
    "    \"GRENDAY_\": [\"nan\", \"continous\"],\n",
    "    \"MAXVO2_\": [99900, \"continous\"],\n",
    "    \"METVL11_\": [\"nan\", \"continous\"],\n",
    "    \"METVL21_\": [\"nan\", \"continous\"],\n",
    "    \"ORNGDAY_\": [\"nan\", \"continous\"],\n",
    "    \"PA1MIN_\": [\"nan\", \"continous\"],\n",
    "    \"PA1VIGM_\": [\"nan\", \"continous\"],\n",
    "    \"PADUR1_\": [\"nan\", \"continous\"],\n",
    "    \"PADUR2_\": [\"nan\", \"continous\"],\n",
    "    \"PAFREQ1_\": [99000, \"continous\"],\n",
    "    \"PAFREQ2_\": [99000, \"continous\"],\n",
    "    \"PAMIN11_\": [\"nan\", \"continous\"],\n",
    "    \"PAMIN21_\": [\"nan\", \"continous\"],\n",
    "    \"PAMISS1_\": [9, \"discrete\"],\n",
    "    \"PAVIG11_\": [\"nan\", \"continous\"],\n",
    "    \"PAVIG21_\": [\"nan\", \"continous\"],\n",
    "    \"VEGEDA1_\": [\"nan\", \"continous\"],\n",
    "    \"_AGE80\": [\"nan\", \"continous\"],\n",
    "    \"_AIDTST3\": [9, \"discrete\"],\n",
    "    \"_ASTHMS1\": [9, \"discrete\"],\n",
    "    \"_BMI5\": [\"nan\", \"continous\"],\n",
    "    \"_CASTHM1\": [9, \"discrete\"],\n",
    "    \"_CHLDCNT\": [9, \"discrete\"],\n",
    "    \"_CHOLCHK\": [9, \"discrete\"],\n",
    "    \"_DRDXAR1\": [\"nan\", \"discrete\"],\n",
    "    \"_DRNKWEK\": [99900, \"continous\"],\n",
    "    \"_EDUCAG\": [9, \"discrete\"],\n",
    "    \"_FLSHOT6\": [9, \"discrete\"],\n",
    "    \"_FRT16\": [0, \"discrete\"],\n",
    "    \"_FRTLT1\": [9, \"discrete\"]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "VC calculated but not used because\n",
    "    - not present in out dataset: '_MICHD', '_VEGRESPFTJUDA1_', 'PAINACT2'\n",
    "    - make no sense: \n",
    "        \"_LLCPWT\" \"_DUALCOR\" \"_DUALUSE\" --> HIDDEN or cell phone\n",
    "        (\"_AGE65YR\" \"_AGEG5YR\" _AGE_G--> alredy rpesent in AGE80) \n",
    "        \"_BMI5CAT\" --> altry in _BMI\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Maybe add other cols that could be useful\n",
    "other_useful_cols = np.array([\"Id\"])\n",
    "\n",
    "# Columns to select\n",
    "useful_cols = np.concatenate([other_useful_cols, list(data_dict.keys())])\n",
    "\n",
    "# Select usduful cols\n",
    "df_filtered = df[useful_cols].copy()\n",
    "print(len(df_filtered.columns))\n",
    "\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to be dropped: []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ACTIN11_</th>\n",
       "      <th>ACTIN21_</th>\n",
       "      <th>DROCDY3_</th>\n",
       "      <th>FC60_</th>\n",
       "      <th>FRUTDA1_</th>\n",
       "      <th>_FRTRESP</th>\n",
       "      <th>_FRUITEX</th>\n",
       "      <th>_FRUTSUM</th>\n",
       "      <th>_HCVU651</th>\n",
       "      <th>...</th>\n",
       "      <th>_BMI5</th>\n",
       "      <th>_CASTHM1</th>\n",
       "      <th>_CHLDCNT</th>\n",
       "      <th>_CHOLCHK</th>\n",
       "      <th>_DRDXAR1</th>\n",
       "      <th>_DRNKWEK</th>\n",
       "      <th>_EDUCAG</th>\n",
       "      <th>_FLSHOT6</th>\n",
       "      <th>_FRT16</th>\n",
       "      <th>_FRTLT1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>20.78</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>4.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>28.70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>99900</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.03</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>27.96</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>900</td>\n",
       "      <td>999.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>24.39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>99900</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328130</th>\n",
       "      <td>328130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71</td>\n",
       "      <td>4.91</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>24.69</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328131</th>\n",
       "      <td>328131</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>24.39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328132</th>\n",
       "      <td>328132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>58.53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328133</th>\n",
       "      <td>328133</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.23</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>35.87</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328134</th>\n",
       "      <td>328134</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.99</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>22.81</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>328135 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id  ACTIN11_  ACTIN21_  DROCDY3_   FC60_  FRUTDA1_  _FRTRESP  \\\n",
       "0            0       1.0       1.0         0    4.61      0.71         1   \n",
       "1            1       NaN       NaN       100    4.91       NaN         0   \n",
       "2            2       1.0       0.0        13    4.11      2.00         1   \n",
       "3            3       2.0       2.0         0    4.11      0.00         1   \n",
       "4            4       NaN       NaN       900  999.00      0.50         1   \n",
       "...        ...       ...       ...       ...     ...       ...       ...   \n",
       "328130  328130       1.0       1.0        71    4.91      1.00         1   \n",
       "328131  328131       2.0       2.0         0    5.10      1.00         1   \n",
       "328132  328132       1.0       0.0         0    3.92      1.00         1   \n",
       "328133  328133       2.0       0.0         0    6.23      0.07         1   \n",
       "328134  328134       1.0       0.0         0    4.99      3.00         1   \n",
       "\n",
       "        _FRUITEX  _FRUTSUM  _HCVU651  ...  _BMI5  _CASTHM1  _CHLDCNT  \\\n",
       "0              0      0.71         1  ...  20.78         1         1   \n",
       "1              1       NaN         1  ...  28.70         1         1   \n",
       "2              0      2.03         9  ...    NaN         1         1   \n",
       "3              0      0.00         9  ...  27.96         1         1   \n",
       "4              0      1.50         9  ...  24.39         1         1   \n",
       "...          ...       ...       ...  ...    ...       ...       ...   \n",
       "328130         0      1.00         1  ...  24.69         1         1   \n",
       "328131         0      1.00         1  ...  24.39         1         1   \n",
       "328132         0      1.00         9  ...  58.53         1         1   \n",
       "328133         0      0.07         1  ...  35.87         2         1   \n",
       "328134         0      4.00         1  ...  22.81         1         2   \n",
       "\n",
       "        _CHOLCHK  _DRDXAR1  _DRNKWEK  _EDUCAG  _FLSHOT6  _FRT16  _FRTLT1  \n",
       "0              1       2.0         0        3       NaN       1        2  \n",
       "1              1       2.0     99900        2       NaN       1        9  \n",
       "2              1       1.0        93        4       1.0       1        1  \n",
       "3              2       1.0         0        2       2.0       1        2  \n",
       "4              1       2.0     99900        9       9.0       1        1  \n",
       "...          ...       ...       ...      ...       ...     ...      ...  \n",
       "328130         1       2.0      1000        4       NaN       1        1  \n",
       "328131         9       2.0         0        3       NaN       1        1  \n",
       "328132         1       2.0         0        3       2.0       1        1  \n",
       "328133         1       2.0         0        2       NaN       1        2  \n",
       "328134         1       1.0         0        4       NaN       1        1  \n",
       "\n",
       "[328135 rows x 80 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REMOVE COLS WITH TOO MANY NAN\n",
    "\n",
    "thr_nan = 0.75\n",
    "\n",
    "# Calculate the threshold for NaN values\n",
    "threshold = thr_nan * len(df_filtered)\n",
    "\n",
    "# Identify columns to drop\n",
    "cols_to_drop = df_filtered.columns[df_filtered.isna().sum() > threshold]\n",
    "\n",
    "# Print the names of the columns that will be dropped\n",
    "print(\"Columns to be dropped:\", cols_to_drop.tolist())\n",
    "\n",
    "# Drop the columns\n",
    "df_filtered.drop(columns=cols_to_drop, inplace=True)\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE PATIENTS WITH TOO MANY NaN\n",
    "\n",
    "# NO !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Id not in the dict.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86161/2114852876.py:21: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_filtered[col_name] = df_filtered[col_name].fillna(mean_value)  # Replace NaN with Mean\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ACTIN11_</th>\n",
       "      <th>ACTIN21_</th>\n",
       "      <th>DROCDY3_</th>\n",
       "      <th>FC60_</th>\n",
       "      <th>FRUTDA1_</th>\n",
       "      <th>_FRTRESP</th>\n",
       "      <th>_FRUITEX</th>\n",
       "      <th>_FRUTSUM</th>\n",
       "      <th>_HCVU651</th>\n",
       "      <th>...</th>\n",
       "      <th>_BMI5</th>\n",
       "      <th>_CASTHM1</th>\n",
       "      <th>_CHLDCNT</th>\n",
       "      <th>_CHOLCHK</th>\n",
       "      <th>_DRDXAR1</th>\n",
       "      <th>_DRNKWEK</th>\n",
       "      <th>_EDUCAG</th>\n",
       "      <th>_FLSHOT6</th>\n",
       "      <th>_FRT16</th>\n",
       "      <th>_FRTLT1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>20.780000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>4.91</td>\n",
       "      <td>1.004961</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.361799</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>28.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>262.715064</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.030000</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>28.033007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>27.960000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16.102383</td>\n",
       "      <td>999.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>24.390000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>262.715064</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328130</th>\n",
       "      <td>328130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>4.91</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>24.690000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328131</th>\n",
       "      <td>328131</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>24.390000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328132</th>\n",
       "      <td>328132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>58.530000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328133</th>\n",
       "      <td>328133</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.23</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>35.870000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328134</th>\n",
       "      <td>328134</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.99</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>22.810000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>328135 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id  ACTIN11_  ACTIN21_    DROCDY3_   FC60_  FRUTDA1_  _FRTRESP  \\\n",
       "0            0       1.0       1.0    0.000000    4.61  0.710000         1   \n",
       "1            1      -1.0      -1.0  100.000000    4.91  1.004961         0   \n",
       "2            2       1.0       0.0   13.000000    4.11  2.000000         1   \n",
       "3            3       2.0       2.0    0.000000    4.11  0.000000         1   \n",
       "4            4      -1.0      -1.0   16.102383  999.00  0.500000         1   \n",
       "...        ...       ...       ...         ...     ...       ...       ...   \n",
       "328130  328130       1.0       1.0   71.000000    4.91  1.000000         1   \n",
       "328131  328131       2.0       2.0    0.000000    5.10  1.000000         1   \n",
       "328132  328132       1.0       0.0    0.000000    3.92  1.000000         1   \n",
       "328133  328133       2.0       0.0    0.000000    6.23  0.070000         1   \n",
       "328134  328134       1.0       0.0    0.000000    4.99  3.000000         1   \n",
       "\n",
       "        _FRUITEX  _FRUTSUM  _HCVU651  ...      _BMI5  _CASTHM1  _CHLDCNT  \\\n",
       "0              0  0.710000         1  ...  20.780000         1         1   \n",
       "1              1  1.361799         1  ...  28.700000         1         1   \n",
       "2              0  2.030000         9  ...  28.033007         1         1   \n",
       "3              0  0.000000         9  ...  27.960000         1         1   \n",
       "4              0  1.500000         9  ...  24.390000         1         1   \n",
       "...          ...       ...       ...  ...        ...       ...       ...   \n",
       "328130         0  1.000000         1  ...  24.690000         1         1   \n",
       "328131         0  1.000000         1  ...  24.390000         1         1   \n",
       "328132         0  1.000000         9  ...  58.530000         1         1   \n",
       "328133         0  0.070000         1  ...  35.870000         2         1   \n",
       "328134         0  4.000000         1  ...  22.810000         1         2   \n",
       "\n",
       "        _CHOLCHK  _DRDXAR1     _DRNKWEK  _EDUCAG  _FLSHOT6  _FRT16  _FRTLT1  \n",
       "0              1       2.0     0.000000        3       9.0       1        2  \n",
       "1              1       2.0   262.715064        2       9.0       1        9  \n",
       "2              1       1.0    93.000000        4       1.0       1        1  \n",
       "3              2       1.0     0.000000        2       2.0       1        2  \n",
       "4              1       2.0   262.715064        9       9.0       1        1  \n",
       "...          ...       ...          ...      ...       ...     ...      ...  \n",
       "328130         1       2.0  1000.000000        4       9.0       1        1  \n",
       "328131         9       2.0     0.000000        3       9.0       1        1  \n",
       "328132         1       2.0     0.000000        3       2.0       1        1  \n",
       "328133         1       2.0     0.000000        2       9.0       1        2  \n",
       "328134         1       1.0     0.000000        4       9.0       1        1  \n",
       "\n",
       "[328135 rows x 80 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# IMPUTE MISSING VALUES\n",
    "\n",
    "for col_name in df_filtered.columns:\n",
    "    try:\n",
    "        col = data_dict[col_name]\n",
    "    except KeyError:\n",
    "        print(f\"Column {col_name} not in the dict.\")\n",
    "        continue  # Skip to the next iteration if the column is not found\n",
    "\n",
    "    type_feature = col[1]\n",
    "    nan_symbol = col[0]\n",
    "\n",
    "    #print(f\"{col_name} {type_feature} {nan_symbol}\")\n",
    "\n",
    "    if type_feature == \"continous\":\n",
    "        # Replace NaN values with the mean of the feature\n",
    "            # but the mean shpodun not be calculayed using the NaN-symols, otherwise the men will be skewed\n",
    "        df_filtered[col_name] = df_filtered[col_name].replace(nan_symbol, None)  # Replace NaN symbols with Mean\n",
    "            # in these way \"9999\" will not be used for clauclatignthe means\n",
    "        mean_value = df_filtered[col_name].mean() # ATTENTION TO NOT OCUNT NAN\n",
    "        df_filtered[col_name] = df_filtered[col_name].fillna(mean_value)  # Replace NaN with Mean\n",
    "        \n",
    "    elif type_feature == \"discrete\":\n",
    "        # Handle NaN values for discrete features\n",
    "        if nan_symbol == \"nan\":  # No predefined category for missing\n",
    "            df_filtered[col_name] = df_filtered[col_name].fillna(-1)  # Replace with -1 for missing\n",
    "        else:\n",
    "            df_filtered[col_name] = df_filtered[col_name].fillna(nan_symbol)  # Replace NaN with predefined missing value\n",
    "\n",
    "display(df_filtered)\n",
    "\n",
    "#Check for Nan Values\n",
    "print(df_filtered.columns[df_filtered.isna().any()].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "Before encoding check if some cols are correlated.\n",
    "Maybe remove the ones highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correlation_matrix = df_filtered.corr()\n",
    "\n",
    "# Set the size of the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_filtered.corr()\n",
    "\n",
    "# Create a mask for correlations greater than 0.6\n",
    "mask = (correlation_matrix.abs() > 0.9) & (correlation_matrix != 1)\n",
    "\n",
    "# Get the pairs of columns with high correlation\n",
    "high_corr_pairs = []\n",
    "\n",
    "for row in correlation_matrix.index:\n",
    "    for col in correlation_matrix.columns:\n",
    "        if mask.loc[row, col]:\n",
    "            high_corr_pairs.append((row, col, correlation_matrix.loc[row, col]))\n",
    "\n",
    "# Print the pairs\n",
    "for pair in high_corr_pairs:\n",
    "    print(f\"Columns: {pair[0]} and {pair[1]} - Correlation: {pair[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Discrete Variables\n",
    "Chose one hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACTIN11_', 'ACTIN21_', '_FRTRESP', '_FRUITEX', '_HCVU651', '_HISPANC', '_INCOMG', '_LMTACT1', '_LMTSCL1', '_LMTWRK1', '_LTASTH1', '_MISFRTN', '_MISVEGN', '_MRACE1', '_PA30021', '_PA150R2', '_PA300R2', '_PACAT1', '_PAINDX1', '_PAREC1', '_PASTAE1', '_PASTRNG', '_PNEUMO2', '_PRACE1', '_RACE', '_RACEG21', '_RACEGR3', '_RACE_G1', '_RFBING5', '_RFBMI5', '_RFCHOL', '_RFDRHV5', '_RFHLTH', '_RFHYPE5', '_RFSEAT2', '_RFSEAT3', '_RFSMOK3', '_SMOKER3', '_TOTINDA', '_VEG23', '_VEGETEX', '_VEGLT1', 'PAMISS1_', '_AIDTST3', '_ASTHMS1', '_CASTHM1', '_CHLDCNT', '_CHOLCHK', '_DRDXAR1', '_EDUCAG', '_FLSHOT6', '_FRT16', '_FRTLT1']\n",
      "182\n"
     ]
    }
   ],
   "source": [
    "# ENCODING OF DISCRETE VARIABLES\n",
    "\n",
    "# Extracting keys where the value indicates \"discrete\"\n",
    "discrete_keys = [key for key, value in data_dict.items() if value[1] == \"discrete\"]\n",
    "print(discrete_keys)\n",
    "#df_filtered = \n",
    "df_filtered = pd.get_dummies(df_filtered, columns=discrete_keys, drop_first=True)\n",
    "    #ATTENTION DROP FISRT\n",
    "print(len(df_filtered.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Countinous Variables\n",
    "\n",
    "ATTENTION:\n",
    "- test and train must be normlized separated (aboid info leckage)\n",
    "- normalization happens along feautures, i.e. each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values before normalization:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DROCDY3_</th>\n",
       "      <th>FC60_</th>\n",
       "      <th>FRUTDA1_</th>\n",
       "      <th>_FRUTSUM</th>\n",
       "      <th>_MINAC11</th>\n",
       "      <th>_MINAC21</th>\n",
       "      <th>_VEGESUM</th>\n",
       "      <th>GRENDAY_</th>\n",
       "      <th>MAXVO2_</th>\n",
       "      <th>METVL11_</th>\n",
       "      <th>...</th>\n",
       "      <th>PAFREQ1_</th>\n",
       "      <th>PAFREQ2_</th>\n",
       "      <th>PAMIN11_</th>\n",
       "      <th>PAMIN21_</th>\n",
       "      <th>PAVIG11_</th>\n",
       "      <th>PAVIG21_</th>\n",
       "      <th>VEGEDA1_</th>\n",
       "      <th>_AGE80</th>\n",
       "      <th>_BMI5</th>\n",
       "      <th>_DRNKWEK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>26.91</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.667000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>57</td>\n",
       "      <td>20.780000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>4.91</td>\n",
       "      <td>1.004961</td>\n",
       "      <td>1.361799</td>\n",
       "      <td>229.539738</td>\n",
       "      <td>129.760626</td>\n",
       "      <td>1.949433</td>\n",
       "      <td>0.567969</td>\n",
       "      <td>28.65</td>\n",
       "      <td>4.170547</td>\n",
       "      <td>...</td>\n",
       "      <td>4.027582</td>\n",
       "      <td>3.128048</td>\n",
       "      <td>313.132584</td>\n",
       "      <td>181.398664</td>\n",
       "      <td>82.10056</td>\n",
       "      <td>51.004438</td>\n",
       "      <td>0.802999</td>\n",
       "      <td>57</td>\n",
       "      <td>28.700000</td>\n",
       "      <td>262.715064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.030000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.860000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.95</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.128048</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>65</td>\n",
       "      <td>28.033007</td>\n",
       "      <td>93.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.95</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>600.00000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>65</td>\n",
       "      <td>27.960000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.102383</td>\n",
       "      <td>999.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>129.760626</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>999.00</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.667000</td>\n",
       "      <td>3.128048</td>\n",
       "      <td>313.132584</td>\n",
       "      <td>181.398664</td>\n",
       "      <td>82.10056</td>\n",
       "      <td>51.004438</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>61</td>\n",
       "      <td>24.390000</td>\n",
       "      <td>262.715064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328130</th>\n",
       "      <td>71.000000</td>\n",
       "      <td>4.91</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>1.890000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>28.65</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>57</td>\n",
       "      <td>24.690000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328131</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>2.340000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>29.75</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.467000</td>\n",
       "      <td>900.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>450.00000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>55</td>\n",
       "      <td>24.390000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328132</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>22.84</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.128048</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>68</td>\n",
       "      <td>58.530000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328133</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.23</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>36.35</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467000</td>\n",
       "      <td>3.128048</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>43</td>\n",
       "      <td>35.870000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328134</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.99</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>229.539738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.13</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>313.132584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>51</td>\n",
       "      <td>22.810000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>328135 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          DROCDY3_   FC60_  FRUTDA1_  _FRUTSUM    _MINAC11    _MINAC21  \\\n",
       "0         0.000000    4.61  0.710000  0.710000   60.000000  140.000000   \n",
       "1       100.000000    4.91  1.004961  1.361799  229.539738  129.760626   \n",
       "2        13.000000    4.11  2.000000  2.030000   90.000000    0.000000   \n",
       "3         0.000000    4.11  0.000000  0.000000  600.000000  180.000000   \n",
       "4        16.102383  999.00  0.500000  1.500000  280.000000  129.760626   \n",
       "...            ...     ...       ...       ...         ...         ...   \n",
       "328130   71.000000    4.91  1.000000  1.000000   75.000000  120.000000   \n",
       "328131    0.000000    5.10  1.000000  1.000000  450.000000   21.000000   \n",
       "328132    0.000000    3.92  1.000000  1.000000  120.000000    0.000000   \n",
       "328133    0.000000    6.23  0.070000  0.070000   14.000000    0.000000   \n",
       "328134    0.000000    4.99  3.000000  4.000000  229.539738    0.000000   \n",
       "\n",
       "        _VEGESUM  GRENDAY_  MAXVO2_  METVL11_  ...  PAFREQ1_  PAFREQ2_  \\\n",
       "0       1.210000  0.100000    26.91  3.500000  ...  2.000000  4.667000   \n",
       "1       1.949433  0.567969    28.65  4.170547  ...  4.027582  3.128048   \n",
       "2       2.860000  1.000000    23.95  3.500000  ...  3.000000  3.128048   \n",
       "3       2.780000  1.000000    23.95  4.500000  ...  5.000000  1.000000   \n",
       "4       1.730000  0.130000   999.00  3.500000  ...  4.667000  3.128048   \n",
       "...          ...       ...      ...       ...  ...       ...       ...   \n",
       "328130  1.890000  0.330000    28.65  3.500000  ...  3.000000  1.000000   \n",
       "328131  2.340000  1.000000    29.75  6.800000  ...  5.000000  0.467000   \n",
       "328132  0.780000  0.070000    22.84  3.500000  ...  4.000000  3.128048   \n",
       "328133  0.360000  0.170000    36.35  7.300000  ...  0.467000  3.128048   \n",
       "328134  7.000000  3.000000    29.13  3.500000  ...  4.000000  3.000000   \n",
       "\n",
       "           PAMIN11_    PAMIN21_   PAVIG11_    PAVIG21_  VEGEDA1_  _AGE80  \\\n",
       "0         60.000000  140.000000    0.00000    0.000000  0.710000      57   \n",
       "1        313.132584  181.398664   82.10056   51.004438  0.802999      57   \n",
       "2         90.000000    0.000000    0.00000    0.000000  1.000000      65   \n",
       "3       1200.000000  360.000000  600.00000  180.000000  1.000000      65   \n",
       "4        313.132584  181.398664   82.10056   51.004438  0.830000      61   \n",
       "...             ...         ...        ...         ...       ...     ...   \n",
       "328130    75.000000  120.000000    0.00000    0.000000  1.000000      57   \n",
       "328131   900.000000   42.000000  450.00000   21.000000  0.200000      55   \n",
       "328132   120.000000    0.000000    0.00000    0.000000  0.570000      68   \n",
       "328133    28.000000    0.000000   14.00000    0.000000  0.130000      43   \n",
       "328134   313.132584    0.000000    0.00000    0.000000  2.000000      51   \n",
       "\n",
       "            _BMI5     _DRNKWEK  \n",
       "0       20.780000     0.000000  \n",
       "1       28.700000   262.715064  \n",
       "2       28.033007    93.000000  \n",
       "3       27.960000     0.000000  \n",
       "4       24.390000   262.715064  \n",
       "...           ...          ...  \n",
       "328130  24.690000  1000.000000  \n",
       "328131  24.390000     0.000000  \n",
       "328132  58.530000     0.000000  \n",
       "328133  35.870000     0.000000  \n",
       "328134  22.810000     0.000000  \n",
       "\n",
       "[328135 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
      "/tmp/ipykernel_86161/336054439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ACTIN11_</th>\n",
       "      <th>ACTIN21_</th>\n",
       "      <th>DROCDY3_</th>\n",
       "      <th>FC60_</th>\n",
       "      <th>FRUTDA1_</th>\n",
       "      <th>_FRTRESP</th>\n",
       "      <th>_FRUITEX</th>\n",
       "      <th>_FRUTSUM</th>\n",
       "      <th>_HCVU651</th>\n",
       "      <th>...</th>\n",
       "      <th>_BMI5</th>\n",
       "      <th>_CASTHM1</th>\n",
       "      <th>_CHLDCNT</th>\n",
       "      <th>_CHOLCHK</th>\n",
       "      <th>_DRDXAR1</th>\n",
       "      <th>_DRNKWEK</th>\n",
       "      <th>_EDUCAG</th>\n",
       "      <th>_FLSHOT6</th>\n",
       "      <th>_FRT16</th>\n",
       "      <th>_FRTLT1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003667</td>\n",
       "      <td>0.007172</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004733</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102301</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.010151</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009079</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194792</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013533</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186150</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.161024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144459</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328130</th>\n",
       "      <td>328130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147962</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.018797</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328131</th>\n",
       "      <td>328131</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144459</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328132</th>\n",
       "      <td>328132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543151</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328133</th>\n",
       "      <td>328133</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005290</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278524</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328134</th>\n",
       "      <td>328134</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004048</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126007</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>328135 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id  ACTIN11_  ACTIN21_  DROCDY3_     FC60_  FRUTDA1_  _FRTRESP  \\\n",
       "0            0       1.0       1.0  0.000000  0.003667  0.007172         1   \n",
       "1            1      -1.0      -1.0  1.000000  0.003968  0.010151         0   \n",
       "2            2       1.0       0.0  0.130000  0.003166  0.020202         1   \n",
       "3            3       2.0       2.0  0.000000  0.003166  0.000000         1   \n",
       "4            4      -1.0      -1.0  0.161024  1.000000  0.005051         1   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "328130  328130       1.0       1.0  0.710000  0.003968  0.010101         1   \n",
       "328131  328131       2.0       2.0  0.000000  0.004158  0.010101         1   \n",
       "328132  328132       1.0       0.0  0.000000  0.002976  0.010101         1   \n",
       "328133  328133       2.0       0.0  0.000000  0.005290  0.000707         1   \n",
       "328134  328134       1.0       0.0  0.000000  0.004048  0.030303         1   \n",
       "\n",
       "        _FRUITEX  _FRUTSUM  _HCVU651  ...     _BMI5  _CASTHM1  _CHLDCNT  \\\n",
       "0              0  0.004733         1  ...  0.102301         1         1   \n",
       "1              1  0.009079         1  ...  0.194792         1         1   \n",
       "2              0  0.013533         9  ...  0.187002         1         1   \n",
       "3              0  0.000000         9  ...  0.186150         1         1   \n",
       "4              0  0.010000         9  ...  0.144459         1         1   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "328130         0  0.006667         1  ...  0.147962         1         1   \n",
       "328131         0  0.006667         1  ...  0.144459         1         1   \n",
       "328132         0  0.006667         9  ...  0.543151         1         1   \n",
       "328133         0  0.000467         1  ...  0.278524         2         1   \n",
       "328134         0  0.026667         1  ...  0.126007         1         2   \n",
       "\n",
       "        _CHOLCHK  _DRDXAR1  _DRNKWEK  _EDUCAG  _FLSHOT6  _FRT16  _FRTLT1  \n",
       "0              1       2.0  0.000000        3       9.0       1        2  \n",
       "1              1       2.0  0.004938        2       9.0       1        9  \n",
       "2              1       1.0  0.001748        4       1.0       1        1  \n",
       "3              2       1.0  0.000000        2       2.0       1        2  \n",
       "4              1       2.0  0.004938        9       9.0       1        1  \n",
       "...          ...       ...       ...      ...       ...     ...      ...  \n",
       "328130         1       2.0  0.018797        4       9.0       1        1  \n",
       "328131         9       2.0  0.000000        3       9.0       1        1  \n",
       "328132         1       2.0  0.000000        3       2.0       1        1  \n",
       "328133         1       2.0  0.000000        2       9.0       1        2  \n",
       "328134         1       1.0  0.000000        4       9.0       1        1  \n",
       "\n",
       "[328135 rows x 80 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NORMALIZATION OF CONTINOUS VARIABLES\n",
    "\n",
    "continuous_keys = [key for key, value in data_dict.items() if value[1] == \"continous\"]\n",
    "normalization = \"minmax\" #minmax, z-score, nothing, robust_scalar\n",
    "\n",
    "print(\"values before normalization:\\n\")\n",
    "display(df_filtered[continuous_keys])\n",
    "\n",
    "# Normalization Techniques\n",
    "# 1. Min-Max Scaling\n",
    "if normalization == \"minmax\":\n",
    "    for key in continuous_keys:\n",
    "        column = df_filtered[key]\n",
    "        mean_value = np.nanmean(column)  # Calculate mean ignoring NaNs\n",
    "        column[np.isnan(column)] = mean_value  # Replace NaNs with mean\n",
    "\n",
    "        # Min-Max Normalization\n",
    "        min_value = np.min(column)\n",
    "        max_value = np.max(column)\n",
    "        column = (column - min_value) / (max_value - min_value)\n",
    "        \n",
    "        # Update the column in df_filtered\n",
    "        df_filtered[key] = column\n",
    "\n",
    "# 2. Z-score\n",
    "if False:\n",
    "    for key in continuous_keys:\n",
    "        column = df_filtered[key]\n",
    "        mean_value = np.nanmean(column)\n",
    "        std_value = np.nanstd(column)  # Standard deviation ignoring NaNs\n",
    "        column[np.isnan(column)] = mean_value  # Fill NaNs with mean\n",
    "        column = (column - mean_value) / std_value\n",
    "        df_filtered[key] = column\n",
    "\n",
    "# 3. Robust Scalar\n",
    "if False:\n",
    "    for key in continuous_keys:\n",
    "        column = df_filtered[key]\n",
    "        median_value = np.nanmedian(column)\n",
    "        q1 = np.nanpercentile(column, 25)  # 25th percentile\n",
    "        q3 = np.nanpercentile(column, 75)  # 75th percentile\n",
    "        iqr = q3 - q1  # Interquartile range\n",
    "        column[np.isnan(column)] = median_value  # Fill NaNs with median\n",
    "        column = (column - median_value) / iqr\n",
    "        df_filtered[key] = column\n",
    "\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engeneering\n",
    "    - PCA\n",
    "    - combinaing features ex. polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,train_ids,test_ids = load_csv_data(\"data/dataset\", sub_sample=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.3000000e+01 1.1000000e+01 1.1162015e+07 ...           nan\n",
      "            nan 2.0000000e+00]\n",
      " [4.5000000e+01 7.0000000e+00 7.1620150e+06 ...           nan\n",
      "            nan 2.0000000e+00]\n",
      " [4.0000000e+00 2.0000000e+00 2.2520150e+06 ...           nan\n",
      "            nan 2.0000000e+00]\n",
      " [8.0000000e+00 4.0000000e+00 6.0120150e+06 ...           nan\n",
      "            nan 1.0000000e+00]\n",
      " [2.9000000e+01 2.0000000e+00 2.1720150e+06 ...           nan\n",
      "            nan 2.0000000e+00]]\n",
      "[-1 -1 -1 ...  1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[:5])\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dtype('float64') dtype('bool')]\n",
      "int64\n",
      "float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 3.66715094e-03, 7.17171717e-03, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "       [1.00000000e+00, 3.96773709e-03, 1.01511245e-02, ...,\n",
       "        1.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       [1.30000000e-01, 3.16617404e-03, 2.02020202e-02, ...,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [0.00000000e+00, 2.97580282e-03, 1.01010101e-02, ...,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 5.29031612e-03, 7.07070707e-04, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 4.04789339e-03, 3.03030303e-02, ...,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 181)\n"
     ]
    }
   ],
   "source": [
    "d = df_filtered\n",
    "\n",
    "d = d.drop(\"Id\", axis=1)\n",
    "print(d.dtypes.unique())\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "for col in d.select_dtypes(include=['bool']).columns:\n",
    "    d[col] = d[col].astype(int)\n",
    "\n",
    "\n",
    "use = 0\n",
    "\n",
    "if use == 0:\n",
    "    # Use all varibales\n",
    "    x_train = d.to_numpy()\n",
    "if use == 1:\n",
    "    # Only use continuos variables\n",
    "    continuous_keys = [key for key, value in data_dict.items() if value[1] == \"continous\"]\n",
    "    d = d[continuous_keys]\n",
    "    x_train = d.to_numpy()\n",
    "if use == 2:\n",
    "    # Only use BMI and AGE80\n",
    "    d = d[[\"_AGE80\", \"_BMI5\"]]\n",
    "    x_train = d.to_numpy()\n",
    "if use == 3:\n",
    "    # Only use continuos variables\n",
    "    continuous_keys = [key for key, value in data_dict.items() if value[1] == \"continous\"]\n",
    "    d = d[continuous_keys]\n",
    "    x_train = d.to_numpy()\n",
    "    x_train = pca(x_train, variance_threshold=0.9)\n",
    "if use == 4:\n",
    "    # Incre the number of continous feaures using polinormilas\n",
    "    continuous_keys = [key for key, value in data_dict.items() if value[1] == \"continous\"]\n",
    "    d = d[continuous_keys]\n",
    "    x_train = d.to_numpy()\n",
    "    x_train = build_poly(x_train, degree=3)\n",
    "if use == 5:\n",
    "    # Use only continuos with pca\n",
    "    continuous_keys = [key for key, value in data_dict.items() if value[1] == \"continous\"]\n",
    "    d = d[continuous_keys]\n",
    "    x_train = d.to_numpy()\n",
    "    x_train = pca(x_train, variance_threshold=0.8)\n",
    "if use == 6:\n",
    "    #only continous with costant term\n",
    "    continuous_keys = [key for key, value in data_dict.items() if value[1] == \"continous\"]\n",
    "    d = d[continuous_keys]\n",
    "    x_train = d.to_numpy()\n",
    "    x_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "\n",
    "\n",
    "\n",
    "print(y_train.dtype)\n",
    "print(x_train.dtype)\n",
    "display(x_train)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_k_fold_cross_validation(y, tx, k_folds, max_iters, gamma, lambda_ridge, optimization_method, seed=42):\n",
    "    \"\"\"Performs Stratified K-Fold Cross-Validation for gradient descent model training using numpy.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): Labels (target values), shape=(N,)\n",
    "        tx (np.array): Input data, shape=(N, D)\n",
    "        k_folds (int): Number of folds (k) for cross-validation\n",
    "        max_iters (int): Number of iterations for gradient descent\n",
    "        gamma (float): Learning rate for gradient descent\n",
    "\n",
    "    Returns:\n",
    "        dict: Cross-validation metrics for each fold for both training and validation sets\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Shuffle the data\n",
    "    indices = np.arange(len(y))\n",
    "    np.random.shuffle(indices)\n",
    "    y, tx = y[indices], tx[indices]\n",
    "\n",
    "    # Group indices by class\n",
    "    #finds the indices corresponding to that class and stores them in the class_indices dictionary\n",
    "    class_indices = {}\n",
    "    for class_label in np.unique(y):\n",
    "        class_indices[class_label] = np.where(y == class_label)[0]\n",
    "\n",
    "    # Create k folds\n",
    "    # Each sublist will eventually hold the indices for a fold.\n",
    "    folds = [[] for _ in range(k_folds)]\n",
    "\n",
    "    for class_label, indices in class_indices.items():\n",
    "        np.random.shuffle(indices)  # Shuffle indices for the current class\n",
    "        fold_sizes = [len(indices) // k_folds] * k_folds # number of samples per fold\n",
    "        for i in range(len(indices) % k_folds):  #those remaining samples are distributed across the first few folds.\n",
    "            fold_sizes[i] += 1  # Distribute remaining samples\n",
    "\n",
    "        current_index = 0\n",
    "        #r, indices are assigned to each fold in order, ensuring that the samples from each class are distributed proportionally.\n",
    "        for fold_index in range(k_folds):\n",
    "            folds[fold_index].extend(indices[current_index:current_index + fold_sizes[fold_index]])\n",
    "            current_index += fold_sizes[fold_index]\n",
    "\n",
    "    # Train and validate\n",
    "    fold_metrics = []  # Store metrics for both training and validation sets\n",
    "    best_w = None  # Best model (i.e. the parameters that compose it)\n",
    "    best_accuracy = 0  # Track the best accuracy\n",
    "    best_f1_score = 0\n",
    "\n",
    "    for fold in range(k_folds):\n",
    "        # The current fold is the validation set\n",
    "        val_indices = folds[fold] \n",
    "        # The training set is constructed by concatenating all indices from the other folds\n",
    "        train_indices = np.concatenate([folds[i] for i in range(k_folds) if i != fold])\n",
    "\n",
    "        # Split the dataset into training and validation sets for this fold\n",
    "        x_train_fold, x_val_fold = tx[train_indices], tx[val_indices]\n",
    "        y_train_fold, y_val_fold = y[train_indices], y[val_indices]\n",
    "\n",
    "        # Initialize weights (can also be random)\n",
    "        initial_w = np.zeros(x_train_fold.shape[1])\n",
    "\n",
    "        # Train the model using mean_squared_error_gd\n",
    "        if optimization_method == \"mse_gd\":\n",
    "            w, train_loss = mean_squared_error_gd(y_train_fold, x_train_fold, initial_w, max_iters, gamma)\n",
    "        elif optimization_method == \"ridge\":\n",
    "            w, train_loss = ridge_regression(y_train_fold, x_train_fold, lambda_=lambda_ridge)\n",
    "        elif optimization_method == \"least_squares\":\n",
    "            w, train_loss = least_squares(y_train_fold, x_train_fold)\n",
    "\n",
    "        # Compute prediction for train and test set\n",
    "        val_predictions = np.sign(np.dot(x_val_fold, w)) # ATTENTION: +1, -1\n",
    "        train_predictions = np.sign(np.dot(x_train_fold, w))\n",
    "\n",
    "        # Compute training loss (MSE) --> calculated before\n",
    "        # Compute validation loss (MSE)\n",
    "        val_error = y_val_fold - np.dot(x_val_fold, w)  # Validation error = true_label - predictions\n",
    "        val_loss = calculate_mse(val_error)  # MSE on validation set\n",
    "\n",
    "        # Calculate accuracy on validation set and training set\n",
    "        val_accuracy = np.mean(val_predictions == y_val_fold)\n",
    "        train_accuracy = np.mean(train_predictions == y_train_fold)\n",
    "\n",
    "        # Calculate F1 score on validation set and  training set\n",
    "        val_f1_score = compute_f1_score(y_val_fold, val_predictions)\n",
    "        train_f1_score = compute_f1_score(y_train_fold, train_predictions)\n",
    "\n",
    "        # Append metrics for this fold\n",
    "        fold_metrics.append({\n",
    "            'fold': fold + 1,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_f1_score': val_f1_score,\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'train_f1_score': train_f1_score\n",
    "        })\n",
    "\n",
    "        # Print fold metrics\n",
    "        print(f\"Fold {fold + 1}/{k_folds}: Validation Loss={val_loss}, Accuracy={val_accuracy}, F1={val_f1_score}\")\n",
    "        print(f\"Fold {fold + 1}/{k_folds}: Training Loss={train_loss}, Accuracy={train_accuracy}, F1={train_f1_score}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Update best model if current validation accuracy is better\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_f1_score = val_f1_score\n",
    "            best_w = w\n",
    "    \n",
    "    # Calculate means across folds\n",
    "    mean_val_accuracy = np.mean([metrics['val_accuracy'] for metrics in fold_metrics])\n",
    "    mean_val_f1_score = np.mean([metrics['val_f1_score'] for metrics in fold_metrics])\n",
    "    mean_train_accuracy = np.mean([metrics['train_accuracy'] for metrics in fold_metrics])\n",
    "    mean_train_f1_score = np.mean([metrics['train_f1_score'] for metrics in fold_metrics])\n",
    "\n",
    "\n",
    "    return fold_metrics, best_w, best_accuracy, best_f1_score, mean_val_accuracy, mean_val_f1_score, mean_train_accuracy, mean_train_f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 1/10: loss=0.5, w0=-0.013610765780801993, w1=-0.0013534241050319094\n",
      "GD iter. 2/10: loss=0.2118826929432724, w0=-0.008432744826051324, w1=-0.0008347294497371775\n",
      "GD iter. 3/10: loss=0.16759438688735348, w0=-0.01055479930906077, w1=-0.001027655915919368\n",
      "GD iter. 4/10: loss=0.1604773202457073, w0=-0.009809399101119469, w1=-0.0009284297880922227\n",
      "GD iter. 5/10: loss=0.15909164507881834, w0=-0.010163100794342347, w1=-0.0009312023523771742\n",
      "GD iter. 6/10: loss=0.1586370246436581, w0=-0.010071846873567257, w1=-0.0008846017748446646\n",
      "GD iter. 7/10: loss=0.1583682712112955, w0=-0.010139550057332932, w1=-0.000849493988252868\n",
      "GD iter. 8/10: loss=0.15816095807818872, w0=-0.010132444093963936, w1=-0.0008036530182007906\n",
      "GD iter. 9/10: loss=0.15798812412378113, w0=-0.010143091298501558, w1=-0.0007569633226375899\n",
      "GD iter. 10/10: loss=0.15783982607833344, w0=-0.010136632391065341, w1=-0.000706562398667649\n",
      "Fold 1/10: Validation Loss=0.1629379525235509, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15771027479606478, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "GD iter. 1/10: loss=0.5, w0=-0.01350400448493895, w1=-0.001421675246751482\n",
      "GD iter. 2/10: loss=0.21353907622691448, w0=-0.008163136477608645, w1=-0.0008975347824520587\n",
      "GD iter. 3/10: loss=0.16854835951579586, w0=-0.010227357988391302, w1=-0.0011227792921226587\n",
      "GD iter. 4/10: loss=0.16115459974254134, w0=-0.00935632989536899, w1=-0.0010387414943510767\n",
      "GD iter. 5/10: loss=0.1596779706690672, w0=-0.009626529492406981, w1=-0.0010652301999347572\n",
      "GD iter. 6/10: loss=0.1591797052486665, w0=-0.009433622622459482, w1=-0.0010390115535005922\n",
      "GD iter. 7/10: loss=0.15887827640904292, w0=-0.009411339410239597, w1=-0.001026177619837988\n",
      "GD iter. 8/10: loss=0.158641613189681, w0=-0.009311345026606646, w1=-0.0010021347939621408\n",
      "GD iter. 9/10: loss=0.1584413606638925, w0=-0.009232663356454465, w1=-0.0009777552960359543\n",
      "GD iter. 10/10: loss=0.15826733799159748, w0=-0.009137293497666147, w1=-0.0009497208592635238\n",
      "Fold 2/10: Validation Loss=0.1582914248517203, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.15811367998826642, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "GD iter. 1/10: loss=0.5, w0=-0.01354240014173775, w1=-0.0013703352653299458\n",
      "GD iter. 2/10: loss=0.21361699716453736, w0=-0.008246511807806403, w1=-0.0008487063787267292\n",
      "GD iter. 3/10: loss=0.16856154809779036, w0=-0.010355398685744092, w1=-0.0010545684584660668\n",
      "GD iter. 4/10: loss=0.16113796689768187, w0=-0.009525515893047105, w1=-0.0009586678886773952\n",
      "GD iter. 5/10: loss=0.15965139496018285, w0=-0.009837400559669142, w1=-0.000969622801614933\n",
      "GD iter. 6/10: loss=0.15915208903545164, w0=-0.009684390419039218, w1=-0.0009286261341722154\n",
      "GD iter. 7/10: loss=0.15885414074401416, w0=-0.009701723690710498, w1=-0.0009001521795008157\n",
      "GD iter. 8/10: loss=0.15862350135726372, w0=-0.009640484690636948, w1=-0.000860296689693245\n",
      "GD iter. 9/10: loss=0.1584307250117653, w0=-0.009600120634971317, w1=-0.0008197175724304533\n",
      "GD iter. 10/10: loss=0.15826495610179964, w0=-0.009542549039476784, w1=-0.0007752180010783702\n",
      "Fold 3/10: Validation Loss=0.15840478559028248, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 3/10: Training Loss=0.15811989489731415, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "GD iter. 1/10: loss=0.5, w0=-0.013685146843300348, w1=-0.0012350976226902983\n",
      "GD iter. 2/10: loss=0.21312708563830002, w0=-0.00835554261485568, w1=-0.0007596387889124084\n",
      "GD iter. 3/10: loss=0.16834305364781252, w0=-0.010473713184337699, w1=-0.0009403498231888164\n",
      "GD iter. 4/10: loss=0.16103611133674892, w0=-0.009646641863520114, w1=-0.0008505127654703893\n",
      "GD iter. 5/10: loss=0.15959516988300723, w0=-0.009959906485054624, w1=-0.0008566836983031241\n",
      "GD iter. 6/10: loss=0.15911937331045242, w0=-0.009810589515059498, w1=-0.0008168655878442637\n",
      "GD iter. 7/10: loss=0.15883675827934599, w0=-0.00983021997982409, w1=-0.0007884690157179637\n",
      "GD iter. 8/10: loss=0.15861708681171213, w0=-0.009772219475935253, w1=-0.0007502564222020193\n",
      "GD iter. 9/10: loss=0.1584322912260862, w0=-0.009734753276479791, w1=-0.0007116391385082697\n",
      "GD iter. 10/10: loss=0.15827227204014843, w0=-0.009680359867315776, w1=-0.000669786800450869\n",
      "Fold 4/10: Validation Loss=0.1586102285693456, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15813125963056185, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "GD iter. 1/10: loss=0.5, w0=-0.01360051883975627, w1=-0.0014210597070246004\n",
      "GD iter. 2/10: loss=0.21308057943728254, w0=-0.008365299516739397, w1=-0.0009283809528841696\n",
      "GD iter. 3/10: loss=0.16844966881156162, w0=-0.0104916590316602, w1=-0.0011687096341798007\n",
      "GD iter. 4/10: loss=0.1611760970099828, w0=-0.009711627695788842, w1=-0.001107750346146202\n",
      "GD iter. 5/10: loss=0.15972872689792508, w0=-0.01005338322074954, w1=-0.0011537532478973593\n",
      "GD iter. 6/10: loss=0.15923892543108972, w0=-0.009939756857539391, w1=-0.0011488808658278104\n",
      "GD iter. 7/10: loss=0.15894261686105202, w0=-0.009990898745232124, w1=-0.0011568152759193976\n",
      "GD iter. 8/10: loss=0.1587110233656654, w0=-0.009965211706077903, w1=-0.0011540489217940464\n",
      "GD iter. 9/10: loss=0.1585161222061275, w0=-0.009958735829309001, w1=-0.001150982492756009\n",
      "GD iter. 10/10: loss=0.1583475877902207, w0=-0.009934952722681026, w1=-0.001144499587948043\n",
      "Fold 5/10: Validation Loss=0.15751780058129677, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.15819939756676482, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "GD iter. 1/10: loss=0.5, w0=-0.013401485006879264, w1=-0.001336304456293462\n",
      "GD iter. 2/10: loss=0.21301365716577422, w0=-0.008168931821879118, w1=-0.0008135223858272757\n",
      "GD iter. 3/10: loss=0.1683352743846597, w0=-0.010222322255603756, w1=-0.0010045122908696807\n",
      "GD iter. 4/10: loss=0.1610718107623058, w0=-0.009393873411815088, w1=-0.000902836645049963\n",
      "GD iter. 5/10: loss=0.15964551433330956, w0=-0.009676317180764555, w1=-0.0009059713467264236\n",
      "GD iter. 6/10: loss=0.15917421320725028, w0=-0.009505292405919128, w1=-0.000859990918368824\n",
      "GD iter. 7/10: loss=0.15889202785819248, w0=-0.009496716794964803, w1=-0.0008269435901069907\n",
      "GD iter. 8/10: loss=0.15867086873587044, w0=-0.00941066017225323, w1=-0.0007838080359743199\n",
      "GD iter. 9/10: loss=0.1584836410285022, w0=-0.009342786317788888, w1=-0.0007406874738733857\n",
      "GD iter. 10/10: loss=0.1583207780513181, w0=-0.009256864675377867, w1=-0.0006944801054122467\n",
      "Fold 6/10: Validation Loss=0.15811173551381502, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.15817682382396206, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "GD iter. 1/10: loss=0.5, w0=-0.013697118517831609, w1=-0.0013357136648392584\n",
      "GD iter. 2/10: loss=0.21313424236989384, w0=-0.00863667086989503, w1=-0.0007722733363966044\n",
      "GD iter. 3/10: loss=0.16837277922849855, w0=-0.010912093015524864, w1=-0.0009353147007165397\n",
      "GD iter. 4/10: loss=0.16106673551650558, w0=-0.010280262814255144, w1=-0.0007977305351840893\n",
      "GD iter. 5/10: loss=0.15962282053113622, w0=-0.010766831141876411, w1=-0.0007660562158083332\n",
      "GD iter. 6/10: loss=0.15914546894475304, w0=-0.010793586810487149, w1=-0.0006832023325976516\n",
      "GD iter. 7/10: loss=0.15886322306450273, w0=-0.010982545727640616, w1=-0.0006129100606759076\n",
      "GD iter. 8/10: loss=0.1586454099697302, w0=-0.011091353124320802, w1=-0.0005317961243016505\n",
      "GD iter. 9/10: loss=0.15846350237047163, w0=-0.011216812558005206, w1=-0.00045036752692317715\n",
      "GD iter. 10/10: loss=0.15830705449931345, w0=-0.011322307223067322, w1=-0.00036556322558926164\n",
      "Fold 7/10: Validation Loss=0.15852582852943667, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.15817005144894825, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "GD iter. 1/10: loss=0.5, w0=-0.013728927831685462, w1=-0.0013021966391259957\n",
      "GD iter. 2/10: loss=0.21359743081134813, w0=-0.008490914523700138, w1=-0.0007477361776818561\n",
      "GD iter. 3/10: loss=0.1684817359016897, w0=-0.010708362892193877, w1=-0.0009074850028450699\n",
      "GD iter. 4/10: loss=0.16104292175710727, w0=-0.009960616674574629, w1=-0.0007708073202175195\n",
      "GD iter. 5/10: loss=0.1595563477733085, w0=-0.010361813573517157, w1=-0.0007393478534740527\n",
      "GD iter. 6/10: loss=0.1590606011077946, w0=-0.01029134451691587, w1=-0.0006570156864846964\n",
      "GD iter. 7/10: loss=0.1587666994727629, w0=-0.010390696693411895, w1=-0.0005873062304365818\n",
      "GD iter. 8/10: loss=0.15853985204314652, w0=-0.010408597632711002, w1=-0.0005067189415266876\n",
      "GD iter. 9/10: loss=0.15835044161940282, w0=-0.010445726334719399, w1=-0.0004257808171825798\n",
      "GD iter. 10/10: loss=0.15818759050522266, w0=-0.01046369247220847, w1=-0.00034135661872709995\n",
      "Fold 8/10: Validation Loss=0.15921987436544274, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.15804502295309977, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "GD iter. 1/10: loss=0.5, w0=-0.013549341059293186, w1=-0.0013692839516827558\n",
      "GD iter. 2/10: loss=0.21252613146856758, w0=-0.008331116864562622, w1=-0.0008115319987341767\n",
      "GD iter. 3/10: loss=0.16827056509299934, w0=-0.010404749402806942, w1=-0.0009884754576702495\n",
      "GD iter. 4/10: loss=0.16113106523094764, w0=-0.009609617948547549, w1=-0.0008652622796769012\n",
      "GD iter. 5/10: loss=0.1597212730761345, w0=-0.009912069626852406, w1=-0.0008482436142844028\n",
      "GD iter. 6/10: loss=0.1592444894649496, w0=-0.009767745523030943, w1=-0.0007811129493499048\n",
      "GD iter. 7/10: loss=0.1589554376186745, w0=-0.009781315615721807, w1=-0.0007266100093881618\n",
      "GD iter. 8/10: loss=0.1587295325783474, w0=-0.009718880742477433, w1=-0.0006616990568264929\n",
      "GD iter. 9/10: loss=0.15853967983867806, w0=-0.00967333889346903, w1=-0.00059652908032313\n",
      "GD iter. 10/10: loss=0.15837579811368144, w0=-0.009609882937288115, w1=-0.0005280857104612455\n",
      "Fold 9/10: Validation Loss=0.15749136506565514, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15823195826905748, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "GD iter. 1/10: loss=0.5, w0=-0.01376915371595682, w1=-0.0014204930018140065\n",
      "GD iter. 2/10: loss=0.21341844080918354, w0=-0.00854691537817235, w1=-0.000879926784113825\n",
      "GD iter. 3/10: loss=0.16853647391667229, w0=-0.010770817184550602, w1=-0.0010927733500654352\n",
      "GD iter. 4/10: loss=0.1611897206468327, w0=-0.01004174894011261, w1=-0.0009946075893885122\n",
      "GD iter. 5/10: loss=0.1597369942189078, w0=-0.010454447713178041, w1=-0.0010071927940910358\n",
      "GD iter. 6/10: loss=0.1592573236445814, w0=-0.01040018866619428, w1=-0.0009669318329408782\n",
      "GD iter. 7/10: loss=0.15897313649970515, w0=-0.010514057254111047, w1=-0.0009400913512263964\n",
      "GD iter. 8/10: loss=0.1587528397723899, w0=-0.010547938608961808, w1=-0.0009021213689413016\n",
      "GD iter. 9/10: loss=0.15856798845958886, w0=-0.010600979199685457, w1=-0.000863909431244544\n",
      "GD iter. 10/10: loss=0.15840831696330415, w0=-0.010635470259557545, w1=-0.000822161093939691\n",
      "Fold 10/10: Validation Loss=0.15745641376398878, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1582679428855486, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "0.9147640791476408\n",
      "0\n",
      "[-9.13729350e-03 -9.49720859e-04 -9.61100130e-04 -8.25310001e-04\n",
      " -3.74367577e-04 -2.64167276e-04 -7.96563932e-04 -4.51718992e-04\n",
      " -2.25338490e-03 -2.17080312e-02 -1.81032094e-02 -2.45118134e-04\n",
      " -8.10480315e-04 -6.25364786e-04 -7.93462337e-03 -8.07539799e-03\n",
      " -3.36236344e-03 -2.29109087e-03 -5.14324775e-04 -3.54167811e-04\n",
      " -5.48368907e-04 -2.21514427e-04 -6.40895731e-04 -4.08420504e-02\n",
      " -1.20932799e-02 -1.63835040e-04 -4.36673125e-03 -2.70895121e-02\n",
      " -1.43158602e-02 -1.35682733e-02 -1.74375926e-02 -1.42367076e-02\n",
      " -5.66669562e-02 -1.21139771e-02  0.00000000e+00 -4.86971852e-03\n",
      " -2.97807261e-02 -5.63381376e-02 -1.53666464e-03 -8.91307032e-03\n",
      " -2.35040853e-03 -7.94834344e-03 -2.17204647e-02 -1.57503598e-02\n",
      " -1.81176298e-02 -2.99634542e-02 -3.00944727e-03 -6.52021266e-03\n",
      " -2.14916965e-02 -2.99634542e-02 -3.68186248e-03 -2.16386321e-02\n",
      " -2.99634542e-02 -4.41426696e-03 -1.43438826e-02 -7.08467794e-04\n",
      " -3.58376042e-03 -8.53021669e-03 -2.52933665e-03  7.24567640e-05\n",
      "  1.83920749e-04 -7.83621246e-03 -4.71561423e-03 -1.55956864e-03\n",
      " -1.67219604e-03 -3.78175542e-04 -2.25626831e-03 -4.30408736e-04\n",
      " -1.35406182e-03 -2.45343943e-03 -2.86850845e-02 -8.99632088e-03\n",
      " -1.12330763e-03 -1.43233419e-02 -8.96237902e-03 -1.43617426e-02\n",
      " -1.43233419e-02 -8.99632088e-03 -1.32384349e-02 -1.12330763e-03\n",
      " -1.43233419e-02 -8.99632088e-03 -1.54466495e-02 -8.96237902e-03\n",
      " -1.88478776e-02 -4.31426613e-03 -1.14041591e-02 -9.14643825e-03\n",
      " -3.45663028e-02 -9.14643825e-03 -2.93631372e-02 -9.01478271e-03\n",
      " -6.85164147e-03 -4.10935488e-02 -4.34872839e-03 -1.77911862e-03\n",
      " -1.44592951e-03 -3.29104908e-04 -2.30527632e-03  1.88652205e-04\n",
      " -2.60919441e-05 -1.35406182e-03 -2.45343943e-03 -4.04687982e-03\n",
      " -1.51983472e-03 -1.40104406e-03 -1.17716432e-04 -4.19204035e-04\n",
      " -7.17022967e-04 -1.09061310e-02 -3.26799302e-03 -1.91278331e-02\n",
      " -3.26799302e-03 -4.04687982e-03 -3.45779924e-03 -7.17022967e-04\n",
      " -1.09061310e-02 -3.26799302e-03 -4.63851072e-02 -4.04687982e-03\n",
      " -1.09061310e-02 -3.45779924e-03 -7.17022967e-04 -6.84389582e-03\n",
      " -8.13199604e-03 -3.45777595e-02 -5.56889526e-03 -2.01450927e-02\n",
      " -1.21989656e-02 -3.04532934e-03 -7.82398527e-03 -1.40989412e-02\n",
      "  5.61233933e-04 -2.10353372e-02 -4.57005148e-04 -4.76627660e-03\n",
      " -8.88222122e-03 -9.13236931e-03 -8.88222122e-03 -1.35402023e-02\n",
      " -4.71592176e-03 -3.78849438e-03 -2.27119193e-02 -2.78128899e-02\n",
      " -4.71592176e-03 -1.39189772e-02 -9.14170702e-03 -6.87809333e-02\n",
      " -1.01091716e-02  0.00000000e+00 -8.18939359e-03 -1.01091716e-02\n",
      " -1.85023833e-03 -9.14170702e-03 -3.53112000e-02 -1.34961835e-02\n",
      " -5.22346095e-03 -5.37285829e-02 -2.05397377e-03 -7.77491566e-03\n",
      " -2.05397377e-03 -4.74234254e-03 -6.05022583e-03 -2.87153376e-03\n",
      " -9.52515907e-04  3.77744156e-04 -2.13793627e-03 -3.53410802e-03\n",
      " -8.90421268e-03 -2.29130881e-03 -3.78332884e-02 -2.99634542e-02\n",
      " -1.80161838e-02 -2.13576819e-02 -2.14979497e-02 -1.24009608e-03\n",
      " -7.20480956e-03 -4.15876294e-02 -6.87809333e-02 -1.35444850e-02\n",
      " -1.21139771e-02]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "max_iters = 10 # Number of iterations for gradient descent\n",
    "gamma = 0.1  # Learning rate\n",
    "k_folds = 10  # Number of folds for cross-validation\n",
    "lambda_ridge = 0.1\n",
    "optimization_method = \"mse_gd\" #mse_gd, msg_sgd, least_squares, ridge\n",
    "\n",
    "# Run cross-validation\n",
    "fold_metrics, best_w, best_accuracy, best_f1_score, mean_val_accuracy, mean_val_f1_score, mean_train_accuracy, mean_train_f1_score = stratified_k_fold_cross_validation(\n",
    "    y = y_train, \n",
    "    tx = x_train, \n",
    "    k_folds = k_folds, \n",
    "    max_iters = max_iters, \n",
    "    gamma = gamma, \n",
    "    lambda_ridge = lambda_ridge, \n",
    "    optimization_method = optimization_method\n",
    "    )\n",
    "\n",
    "\n",
    "print(best_accuracy)\n",
    "print(best_f1_score)\n",
    "print(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Plot the accuracy and F1 score for each fold.\"\"\"\n",
    "folds = [metrics['fold'] for metrics in fold_metrics]\n",
    "val_accuracies = [metrics['val_accuracy'] for metrics in fold_metrics]\n",
    "val_f1_scores = [metrics['val_f1_score'] for metrics in fold_metrics]\n",
    "train_accuracies = [metrics['train_accuracy'] for metrics in fold_metrics]\n",
    "train_f1_scores = [metrics['train_f1_score'] for metrics in fold_metrics]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Validation Accuracy and F1 Score\n",
    "plt.plot(folds, val_accuracies, label='Validation Accuracy', marker='o', linestyle='-')\n",
    "plt.plot(folds, val_f1_scores, label='Validation F1 Score', marker='o', linestyle='-')\n",
    "plt.plot(folds, train_accuracies, label='Training Accuracy', marker='o', linestyle='--')\n",
    "plt.plot(folds, train_f1_scores, label='Training F1 Score', marker='o', linestyle='--')\n",
    "\n",
    "plt.title('Metrics by Fold')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.xticks(folds)\n",
    "plt.ylim(0, 1)  # Assuming accuracy and F1 scores are between 0 and 1\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 1/10: Validation Loss=0.16338641936743858, Accuracy=0.9133738601823708, F1=0\n",
      "Fold 1/10: Training Loss=0.15255684958693894, Accuracy=0.9146486028789161, F1=0\n",
      "\n",
      "\n",
      "Fold 2/10: Validation Loss=0.1603542366151655, Accuracy=0.9147640791476408, F1=0\n",
      "Fold 2/10: Training Loss=0.1528178108741451, Accuracy=0.9144937351845581, F1=0\n",
      "\n",
      "\n",
      "Fold 3/10: Validation Loss=0.1626143226835523, Accuracy=0.913109756097561, F1=0\n",
      "Fold 3/10: Training Loss=0.1527526160934711, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 4/10: Validation Loss=0.15891908242798117, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 4/10: Training Loss=0.15295016975975176, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 5/10: Validation Loss=0.16235749931096152, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 5/10: Training Loss=0.1527323531642158, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 6/10: Validation Loss=0.15849321264160557, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 6/10: Training Loss=0.1530155536619282, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 7/10: Validation Loss=0.15876566034286368, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 7/10: Training Loss=0.153177584360666, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 8/10: Validation Loss=0.1567159588270963, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 8/10: Training Loss=0.1532380768912876, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 9/10: Validation Loss=0.16166074011313386, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 9/10: Training Loss=0.15290304552213732, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n",
      "Fold 10/10: Validation Loss=0.15726465816422644, Accuracy=0.9146341463414634, F1=0\n",
      "Fold 10/10: Training Loss=0.1532197179609128, Accuracy=0.9145082105975961, F1=0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_accuracies = []\n",
    "mean_f1_scores = []\n",
    "lambdas = np.logspace(-9, 1, 100)\n",
    "\n",
    "\n",
    "for lambda_ridge in lambdas:\n",
    "    # Perform stratified k-fold cross-validation\n",
    "    fold_metrics, best_w, best_accuracy, best_f1_score, mean_val_accuracy, mean_val_f1_score, mean_train_accuracy, mean_train_f1_score = stratified_k_fold_cross_validation(\n",
    "        y = y_train, \n",
    "        tx = x_train, \n",
    "        k_folds = k_folds, \n",
    "        max_iters = max_iters, \n",
    "        gamma = gamma, \n",
    "        lambda_ridge = 0.1, \n",
    "        optimization_method = \"ridge\"\n",
    "        )\n",
    "    \n",
    "    mean_accuracies.append(mean_val_accuracy)\n",
    "    mean_f1_scores.append(mean_val_f1_score)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(lambdas, mean_accuracies, label='Mean Accuracy', color='blue')\n",
    "#plt.plot(lambdas, mean_f1_scores, label='Mean F1 Score', color='orange')\n",
    "\n",
    "plt.xscale('log')  # Log scale for lambda\n",
    "plt.xlabel('Lambda (Regularization Strength)')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Mean Accuracy and F1 Score vs. Lambda for Ridge Regression')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16632282,  0.06161856],\n",
       "       [-0.80759004, -0.14923779],\n",
       "       [ 0.02728606,  0.17440413],\n",
       "       ...,\n",
       "       [ 0.15372545,  0.2517041 ],\n",
       "       [ 0.23191809, -0.15248983],\n",
       "       [ 0.20013794, -0.01934327]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pca = pca(x_train, variance_threshold=1)\n",
    "x_pca = x_pca[:,:2]\n",
    "display(x_pca)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_pca[:,0], x_pca[:,1], marker='o')\n",
    "plt.xscale('log')  # Set x-axis to logarithmic scale\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "object\n",
      "(328135, 181)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.dtype)\n",
    "print(x_train.dtype)\n",
    "print(x_train.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
